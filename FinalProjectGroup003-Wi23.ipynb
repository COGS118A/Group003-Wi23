{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 118A - Final Project"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Insert title here\n",
    "\n",
    "## Group members\n",
    "\n",
    "- Angkitha Anguraj\n",
    "- Andres Villegas\n",
    "- Hieu Pham\n",
    "- Sujal Nahata\n",
    "- Denny Yoo"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract \n",
    "As time progresses, scientists and civilians alike are growing concerned by the increased frequency of earthquakes. Since earthquakes are a product of built-up tension between tectonic plates, they are often described as hard to predict. However, we believe utilizing what we have learned about supervised learning in combination with past earthquake data will help us magnitude of future earthquakes. This project aims to predict the magnitude of future earthquakes using machine learning techniques; we will use linear regression, regression decision trees, and feedforward neural networks. The dataset utilized is a public dataset from Kaggle that contains significant earthquakes from 1965-2016. We trained these models to describe our earthquake features in relation to the magnitude and predict magnitudes of future earthquakes, ultimately providing us insights into the relationship between earthquake features and magnitudes. This will help us prevent future earthquakes-related catastrophes. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "The National Earthquake Information Center (NEIC) is responsible for determining the location and magnitude of significant earthquakes worldwide and disseminating this information to relevant organizations and the general public<a name=\"usgs\"></a>[<sup>[1]</sup>](#usgsnote). Since 1965, the NEIC has been maintaining a comprehensive earthquake database, which includes information on earthquakes with a magnitude of 5.5 or higher. This database serves as a crucial resource for scientific research and aids in earthquake prediction<a name=\"usgs\"></a>[<sup>[1]</sup>](#usgsnote)<a name=\"neic\"></a>[<sup>[2]</sup>](#neicnote).The NEIC relies on a variety of technologies to detect and locate earthquakes, including seismometers, GPS, and satellite imagery2. Seismometers detect ground motion caused by earthquakes and are capable of detecting earthquakes that occur anywhere on the planet<a name=\"neic\"></a>[<sup>[2]</sup>](#neicnote). GPS technology can detect deformations in the Earth's crust caused by tectonic activity and can help scientists predict future earthquakes2. Satellite imagery can also provide information on changes in the Earth's surface, which may indicate the occurrence of an earthquake<a name=\"neic\"></a>[<sup>[2]</sup>](#neicnote).\n",
    "Earthquake prediction is of utmost importance, as it can help to minimize the damage and loss of life caused by these natural disasters<a name=\"wef\"></a>[<sup>[3]</sup>](#wefnote). Seismologists and geologists have been studying earthquakes for many years, and detecting and reporting them accurately and promptly is a critical part of their work. The NEIC utilizes several techniques to determine the location and magnitude of earthquakes, including analyzing seismic waves and signals detected by seismometers<a name=\"usgs\"></a>[<sup>[1]</sup>](#usgsnote). In recent years, advances in seismometer technology and data processing techniques have allowed for more precise measurements of seismic signals and faster processing of earthquake data<a name=\"wef\"></a>[<sup>[3]</sup>](#wefnote).\n",
    "The NEIC's earthquake database has been instrumental in identifying patterns and trends in earthquake activity, which can help in predicting future earthquakes<a name=\"usgs\"></a>[<sup>[1]</sup>](#usgsnote). The ability to predict earthquakes accurately can aid in emergency management and disaster response, potentially saving many lives and minimizing damage to infrastructure<a name=\"wef\"></a>[<sup>[3]</sup>](#wefnote).\n",
    "Overall, earthquake research remains a critical area of study, and ongoing efforts to improve earthquake detection, reporting, and prediction will help to minimize the impact of these natural disasters on people and communities worldwide<a name=\"wef\"></a>[<sup>[3]</sup>](#wefnote).\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "\n",
    "As the number and intensity of earthquakes has increased, it has become increasingly important to predict the magnitude of future earthquakes to determine whether there are earthquake-related catastrophes in our near future. The goal of our project is to predict the magnitude of future earthquakes; in doing so, we can prepare emergency supplies to those affected and prevent large scale loss of life.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "The dataset can be accessed at this link: https://www.kaggle.com/datasets/usgs/earthquake-database\n",
    "\n",
    "\n",
    "The data was compiled by the United States Geological Survey (USGS), pulling from information provided by the National Earthquake Information Center (NEIC). It focuses on earthquakes that were recorded at 5.5 magnitude or higher from 1965 to 2016. \n",
    "There are over 23,000 data points with 21 different variables in the original dataset. However, the cleaned and prepared dataset consists of 7286 observations and 10 variables.\n",
    "\n",
    "\n",
    "These variables include: latitude, longitude, depth, depth seismic stations, magnitude, magnitude seismic station, azimuthal gap, horizontal distance, root mean square, and grid location.\n",
    "\n",
    "\n",
    "An observation will include data for each of these variables.\n",
    "\n",
    "\n",
    "The critical feature variables will be: depth, depth seismic stations, magnitude, magnitude seismic station, azimuthal gap, horizontal distance, root mean square (a measurement of seismic waves), and grid location. Since all of these variables, in exception to grid location, have continuous numerical values, they are left in their raw form. Grid location is categorical and refers to the latitude-longitude section where each earthquake occurred. The critical classification variable will be magnitude. This is also numeric.\n",
    "\n",
    "\n",
    "Cleaning was required to remove unnecessary columns for variables like ID and source. This is because the values for these variables were either identical across all observations or were obviously irrelevant for predicting magnitudes. Additionally, observations without values for some of the variables or NaN values were removed because they cannot be used in a holistic model. Finally, a new variable, grid location, was made using the input of each observation’s latitude and longitude and fitting it in a categorical grid. For each entry of grid location, the letter represents the latitude boundary and the number represents the longitude boundary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# Remove irrelevant data points and null observations\n",
    "data = pd.read_csv('database.csv')\n",
    "data = data[data['Type'] == 'Earthquake']\n",
    "data = data.fillna(0)\n",
    "\n",
    "# Remove irrelevant feature columns\n",
    "data = data.drop(labels=['Magnitude Error','Date','Time','Depth Error','Horizontal Error','ID',\n",
    "                         'Source','Type','Location Source','Magnitude Source','Status',\n",
    "                         'Depth Seismic Stations', 'Magnitude Seismic Stations', 'Azimuthal Gap', 'Horizontal Distance'],axis=1)\n",
    "\n",
    "# Convert categorical variables into quatifiable ones using one-hot encoding\n",
    "data = pd.get_dummies(data, columns = ['Magnitude Type'], drop_first=True)\n",
    "\n",
    "# Shuffle the data and separate feature data from labels\n",
    "data = shuffle(data).reset_index(drop=True)\n",
    "X = data.drop(labels = ['Magnitude'], axis=1)\n",
    "y = data['Magnitude']\n",
    "\n",
    "data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proposed Solution\n",
    "After receiving feedback from instructors, we decided it would be best to predict only the magnitude of future earthquakes utilizing three models: a linear regression, a regression tree, and a neural network. We chose not to predict other features as the models we studied in class equip us to predict one feature per model. \n",
    "\n",
    "In order to better visualize where earthquakes occur, we converted the longitude and latitude coordinates featured in our dataset into grid locations (analogous to a chess board). We would graph the earthquakes on this grid and utilize the grid coordinate as one of our features. \n",
    "\n",
    "Since magnitudes are an integral in interpreting the danger of earthquakes and are continuous numerical values, we chose to use linear regression; it has the ability to accurately model relationships and predict continuous numerical values. In this case, the features of our model would be the grid coordinate, depth, and azimuthal gap. Using these, we will predict the magnitude as our data point. In order to test the accuracy of this model, we will use mean squared error.\n",
    "\n",
    "On the other hand, we plan on using a regression decision tree. This model is similar to linear regression. Its only difference is that it uses decision partitioning to make its regression. The variables we’ll feed into the decision tree will stay the same as the previous model. Each of these will act as a split for the model. A regression decision tree model is appropriate for our project as it’s good for predicting a singular numerical value. We will then use cross validation to check the accuracy of our model’s results. \n",
    "\n",
    "Finally, after researching, we decided to use feedforward neural networks featured in PyTorch as our neural network model. Since predicting magnitude is a regression based problem and our data is tabular in nature a feedforward neural network works best. We do not need to use a CNN as they’re typically used for image processing or data with spatial relationships. RNNs are off the table as they aren’t applicable to tabular data. Again, the features remain the same as the previous models. Since the size of our dataset is big, we aim to create a model using 40-50% of our dataset. We will train our neural network and fit it to our split dataset. To validate the accuracy of our neural network, we will calculate the loss/error for our batches of data and use functions similar to those within discussion lab eight to calculate the final accuracy. \n",
    "\n",
    "To first clean our data, we will utilize Pandas. To create the aforementioned grid coordinates and graph our data, we will need to import Matplotlib and Seaborn. In addition, given that we mainly utilize regression models, we will need to import Numpy, Scikit-Learn, and PyTorch (and the necessary functions and libraries). All these in conjunction with one another will help us predict the magnitude of future earthquakes and validate the accuracy of our predictions. We will finalize whichever model produces the highest accuracy.  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics\n",
    "\n",
    "Since one of the models we are using is a linear regression model, the most appropriate evaluation metric to use for this is mean squared error. Since the predictions of our model are continuous values, this will give us a quantitative overview of how far off the actual values are from the predicted values.\n",
    "\n",
    "mean squared error = $\\frac{1}{n}\\sum_{i=1}^{n}(y_i-\\hat{y_i})^2 =\\frac{1}{n}\\sum_{i=1}^{n}(y_i-mx_i+b)^2,\\\\$\n",
    "where $mx+b$ is the linear regression model with weight vector m and constant vector b.\n",
    "\n",
    "In addition, we will utilize k-fold cross validation to verify the accuracy of our decision tree results. Cross validation works in that we split our data into chunks and run our decision tree model on all but a specific split number of chunks. These chunks then become the set that we test on; we then run our model as many times as there chunks as we use each chunk to test. We then compare our tests and graph our results to see which split number produced the best results. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "You may have done tons of work on this. Not all of it belongs here. \n",
    "\n",
    "Reports should have a __narrative__. Once you've looked through all your results over the quarter, decide on one main point and 2-4 secondary points you want us to understand. Include the detailed code and analysis results of those points only; you should spend more time/code/plots on your main point than the others.\n",
    "\n",
    "If you went down any blind alleys that you later decided to not pursue, please don't abuse the TAs time by throwing in 81 lines of code and 4 plots related to something you actually abandoned.  Consider deleting things that are not important to your narrative.  If its slightly relevant to the narrative or you just want us to know you tried something, you could keep it in by summarizing the result in this report in a sentence or two, moving the actual analysis to another file in your repo, and providing us a link to that file.\n",
    "\n",
    "### Subsection 1\n",
    "\n",
    "You will likely have different subsections as you go through your report. For instance you might start with an analysis of the dataset/problem and from there you might be able to draw out the kinds of algorithms that are / aren't appropriate to tackle the solution.  Or something else completely if this isn't the way your project works.\n",
    "\n",
    "### Subsection 2\n",
    "\n",
    "Another likely section is if you are doing any feature selection through cross-validation or hand-design/validation of features/transformations of the data\n",
    "\n",
    "### Subsection 3\n",
    "\n",
    "Probably you need to describe the base model and demonstrate its performance.  Maybe you include a learning curve to show whether you have enough data to do train/validate/test split or have to go to k-folds or LOOCV or ???\n",
    "\n",
    "### Subsection 4\n",
    "\n",
    "Perhaps some exploration of the model selection (hyper-parameters) or algorithm selection task. Validation curves, plots showing the variability of perfromance across folds of the cross-validation, etc. If you're doing one, the outcome of the null hypothesis test or parsimony principle check to show how you are selecting the best model.\n",
    "\n",
    "### Subsection 5 \n",
    "\n",
    "Maybe you do model selection again, but using a different kind of metric than before?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries and tools\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.tree import plot_tree\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import copy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_model = linear_model.LinearRegression()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.4, random_state = 42)\n",
    "\n",
    "lin_model.fit(X_train, y_train)\n",
    "y_pred = lin_model.predict(X_test)\n",
    "\n",
    "print('mean_squared_error : ', mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows = 2, ncols = 2, sharex = False, sharey = False)\n",
    "\n",
    "axs[0, 0].scatter(X_train['Depth'], y_train)\n",
    "m,b = np.polyfit(X_train['Depth'], y_train, 1)\n",
    "coef = np.polyfit(X_train['Depth'], y_train,1)\n",
    "poly1d_fn = np.poly1d(coef) \n",
    "axs[0, 0].plot(X_train['Depth'], y_train, 'co', X_train['Depth'], poly1d_fn(X_train['Depth']), '--k')\n",
    "\n",
    "axs[0, 1].scatter(X_train['Latitude'], y_train)\n",
    "m,b = np.polyfit(X_train['Latitude'], y_train, 1)\n",
    "coef = np.polyfit(X_train['Latitude'], y_train,1)\n",
    "poly1d_fn = np.poly1d(coef) \n",
    "axs[0, 1].plot(X_train['Latitude'], y_train, 'co', X_train['Latitude'], poly1d_fn(X_train['Latitude']), '--k')\n",
    "\n",
    "axs[1, 0].scatter(X_train['Longitude'], y_train)\n",
    "m,b = np.polyfit(X_train['Longitude'], y_train, 1)\n",
    "coef = np.polyfit(X_train['Longitude'], y_train,1)\n",
    "poly1d_fn = np.poly1d(coef) \n",
    "axs[1, 0].plot(X_train['Longitude'], y_train, 'co', X_train['Longitude'], poly1d_fn(X_train['Longitude']), '--k')\n",
    "\n",
    "axs[1, 1].scatter(X_train['Root Mean Square'], y_train)\n",
    "m,b = np.polyfit(X_train['Root Mean Square'], y_train, 1)\n",
    "coef = np.polyfit(X_train['Root Mean Square'], y_train,1)\n",
    "poly1d_fn = np.poly1d(coef) \n",
    "axs[1, 1].plot(X_train['Root Mean Square'], y_train, 'co', X_train['Root Mean Square'], poly1d_fn(X_train['Root Mean Square']), '--k')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a decision tree regression model on the training data.\n",
    "treeModel = DecisionTreeRegressor(max_depth = 3)\n",
    "treeModel.fit(X_train, y_train)\n",
    "\n",
    "# Predicting and getting the scores of the model (MSE and R2)\n",
    "y_pred = treeModel.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"MSE: {:.4f}\".format(mse))\n",
    "print(\"R2 score: {:.4f}\".format(r2))\n",
    "\n",
    "# Plot a decision tree based on prediction\n",
    "plt.figure(figsize=(10,8), dpi=150)\n",
    "plot_tree(treeModel, feature_names=X.columns)\n",
    "\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the data to tensors so PyTorch can use the data\n",
    "class Data(Dataset):\n",
    "    def __init__(self, neural_X, y):\n",
    "        self.neural_X = torch.from_numpy(neural_X.astype(np.float32))\n",
    "        self.y = torch.from_numpy(y.astype(np.float32))\n",
    "        self.len = self.neural_X.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.neural_X[index], self.y[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "# Establish fixed settings for the nueral network\n",
    "batch_size = 150\n",
    "num_epochs = 80\n",
    "learning_rate = 0.001\n",
    "num_classes = 1\n",
    "input_size = (X_train.shape[1])\n",
    "hidden_size = 500\n",
    "n_total_batches = 30\n",
    "\n",
    "# Choose training and test data by batches and convert them into tensors\n",
    "train_data = Data(X_train.to_numpy(), y_train.to_numpy())\n",
    "train_dataloader = DataLoader(dataset= train_data, batch_size= batch_size, shuffle=True)\n",
    "test_data = Data(X_test.to_numpy(), y_test.to_numpy())\n",
    "test_dataloader = DataLoader(dataset= test_data, batch_size= batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a model with an input layer, hidden layer, and output layer\n",
    "class NeuralNet(nn.Module):\n",
    "    \n",
    "    # Initializes the neural network and sets its parameters. \n",
    "    # It takes three arguments:\n",
    "    #   input_size - the size of the input layer determined by the amount of features\n",
    "    #   hidden_size - the number of neurons in the hidden layer\n",
    "    #   num_classes - the number of output classes, which is 1 since we're only predicting a continuous magnitude value\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        \n",
    "        # Create the first linear layer\n",
    "        self.l1 = nn.Linear(input_size , hidden_size)\n",
    "        # Use ReLU on the first layer\n",
    "        self.relu = nn.ReLU() \n",
    "        \n",
    "        # Produe the final layer by connecting the hidden layer with 500 neurons to the second layer\n",
    "        self.l2 = nn.Linear(hidden_size, num_classes)\n",
    " \n",
    "\n",
    "    # Define how the input data is processed through the neural network\n",
    "    # Connect each layer together as following:  l1 -> relu -> l2\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.l1(x))\n",
    "        out = self.relu(self.l2(x))\n",
    "        return out\n",
    "    \n",
    "\n",
    "# Create an instance of NeuralNet and store it in model\n",
    "model = NeuralNet(input_size, hidden_size, num_classes)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use mean squared error (MSE) to evaluate the loss on the model\n",
    "# Instantiate optimizer with a specified learning rate\n",
    "loss_func = nn.MSELoss()  \n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the supported device\n",
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda:0')\n",
    "    else:\n",
    "        device = torch.device('cpu') # use the CPU if the GPU isn't available\n",
    "    return device\n",
    "\n",
    "# Convert a dataframe to tensors to be used with PyTorch\n",
    "def df_to_tensor(df):\n",
    "    device = get_device()\n",
    "    return torch.from_numpy(df.values).float().to(device)\n",
    "\n",
    "# Convert data to tensors to train the model\n",
    "neural_X_train = df_to_tensor(X_train)\n",
    "neural_y_train = df_to_tensor(y_train).reshape(-1, 1)\n",
    "neural_X_test = df_to_tensor(X_test)\n",
    "neural_y_test = df_to_tensor(y_test).reshape(-1, 1)\n",
    "\n",
    "# Instantiating the optimal mse, weights, and losses vectors\n",
    "optimal_mse = np.inf\n",
    "optimal_weights = None\n",
    "training_losses = []\n",
    "validation_losses = []\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for i in range(n_total_batches): # Dataloader pulls a batch and uses it for training\n",
    "        X_batch = neural_X_train[i:i+batch_size]\n",
    "        y_batch = neural_y_train[i:i+batch_size]\n",
    "        \n",
    "        y_pred = model(X_batch)\n",
    "        loss = loss_func(y_pred, y_batch)\n",
    "        \n",
    "        # Perform a backward pass and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Evaluate accuracy using training loss\n",
    "        model.eval()\n",
    "        y_pred = model(neural_X_train)\n",
    "        mse = float(loss_func(y_pred, neural_y_train))\n",
    "        training_losses.append(mse)\n",
    "        if (i+1) % 10 == 0:\n",
    "            print (f'Epoch [{epoch+1}/{num_epochs}], Batch [{i+1}/{n_total_batches}], Training Loss: {mse:.4f}')\n",
    "\n",
    "    # Evaluate accuracy using validation loss\n",
    "    model.eval()\n",
    "    y_pred = model(neural_X_test)\n",
    "    mse = float(loss_func(y_pred, neural_y_test))\n",
    "    validation_losses.append(mse)\n",
    "\n",
    "    # Identify the weights that produce the lowest validation loss\n",
    "    if(mse < optimal_mse):\n",
    "        optimal_mse = mse\n",
    "        optimal_weights = copy.deepcopy(model.state_dict())\n",
    "\n",
    "# Store the optimal weights and print the error with those weights\n",
    "model.load_state_dict(optimal_weights)\n",
    "print(\"Optimal MSE: %.2f\" % optimal_mse)\n",
    "print(\"Optimal RMSE: %.2f\" % np.sqrt(optimal_mse))\n",
    "\n",
    "# Plot the training loss and validation loss against each other\n",
    "plt.plot(training_losses[0:num_epochs], label = \"Training Loss\")\n",
    "plt.plot(validation_losses, label = \"Validation Loss\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.xlabel(\"Number of Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training Loss v. Validation Loss for {} Epochs\".format(num_epochs))\n",
    "plt.show()\n",
    "\n",
    "# Plot the training loss over batches to show convergance\n",
    "plt.plot(training_losses[0:400], label = \"Training Loss\")\n",
    "plt.xlabel(\"Number of Batches\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion\n",
    "\n",
    "### Interpreting the result\n",
    "\n",
    "OK, you've given us quite a bit of tech informaiton above, now its time to tell us what to pay attention to in all that.  Think clearly about your results, decide on one main point and 2-4 secondary points you want us to understand. Highlight HOW your results support those points.  You probably want 2-5 sentences per point.\n",
    "\n",
    "### Limitations\n",
    "\n",
    "Are there any problems with the work?  For instance would more data change the nature of the problem? Would it be good to explore more hyperparams than you had time for?   \n",
    "\n",
    "### Ethics & Privacy\n",
    "\n",
    "If your project has obvious potential concerns with ethics or data privacy discuss that here.  Almost every ML project put into production can have ethical implications if you use your imagination. Use your imagination.\n",
    "\n",
    "Even if you can't come up with an obvious ethical concern that should be addressed, you should know that a large number of ML projects that go into producation have unintended consequences and ethical problems once in production. How will your team address these issues?\n",
    "\n",
    "Consider a tool to help you address the potential issues such as https://deon.drivendata.org\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "Reiterate your main point and in just a few sentences tell us how your results support it. Mention how this work would fit in the background/context of other work in this field if you can. Suggest directions for future work if you want to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Footnotes\n",
    "<a name=\"lorenznote\"></a>1.[^](#lorenz): Lorenz, T. (9 Dec 2021) Birds Aren’t Real, or Are They? Inside a Gen Z Conspiracy Theory. *The New York Times*. https://www.nytimes.com/2021/12/09/technology/birds-arent-real-gen-z-misinformation.html<br> \n",
    "<a name=\"admonishnote\"></a>2.[^](#admonish): Also refs should be important to the background, not some randomly chosen vaguely related stuff. Include a web link if possible in refs as above.<br>\n",
    "<a name=\"sotanote\"></a>3.[^](#sota): Perhaps the current state of the art solution such as you see on [Papers with code](https://paperswithcode.com/sota). Or maybe not SOTA, but rather a standard textbook/Kaggle solution to this kind of problem\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
